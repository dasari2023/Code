{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPaeVJ0skN8YvJwU2yL3Et6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dasari2023/Code/blob/main/EfficientNetIDRiD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzNQEwlnbbs5",
        "outputId": "7c69a01e-1ad1-4986-9dfe-7a0f0501504f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.2)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "import tensorflow\n",
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4GyiT5vbvhq",
        "outputId": "67a0eb18-9918-41f5-8218-c7a3aa1db6ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.backend import image_data_format\n",
        "from keras.models import Sequential\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from keras.models import Sequential\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Conv2D, DepthwiseConv2D, SeparableConv2D\n",
        "from keras.layers import AvgPool2D, MaxPooling2D, Dropout, Flatten, BatchNormalization\n",
        "#from keras.constraints import maxnorm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import copy\n",
        "import random\n",
        "import sys\n",
        "import glob\n",
        "import keras\n",
        "import cv2\n",
        "import csv\n",
        "import time\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "metadata": {
        "id": "yoXYehKtb0bC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pytesseract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9PFGUL6b8qR",
        "outputId": "8a24e293-36be-4d72-80bd-85c419e2ba2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "from pytesseract import image_to_string"
      ],
      "metadata": {
        "id": "soG1rPeJcAhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install opencv-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iV2rfBdQcEjC",
        "outputId": "7e20a049-2720-4d13-d756-2276ac976616"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x,y1, y = [], [],[]\n",
        "\n",
        "x_train=list()\n",
        "y_train=list()\n",
        "\n",
        "dgtrain =glob.glob('/content/drive/MyDrive/Dataset/Disease Grading/Training/*.jpg')\n",
        "ltrain =glob.glob('/content/drive/MyDrive/Dataset/Localization/Training/*.jpg')\n",
        "strain =glob.glob('/content/drive/MyDrive/Dataset/Segmentation/Training/*.jpg')\n",
        "for i in dgtrain:\n",
        "    img=cv2.imread(i,1)\n",
        "    img=cv2.resize(img,(224,224))\n",
        "    img=np.float32(img)\n",
        "    img/=255.0\n",
        "    img = img.reshape(224, 224, 3)\n",
        "    x.append(img)\n",
        "    y.append(0)\n",
        "for j in ltrain:\n",
        "    img=cv2.imread(j,1)\n",
        "    img=cv2.resize(img,(224,224))\n",
        "    img=np.float32(img)\n",
        "    img/=255.0\n",
        "    img = img.reshape(224, 224, 3)\n",
        "    x.append(img)\n",
        "    y.append(1)\n",
        "for k in strain:\n",
        "    img=cv2.imread(k,1)\n",
        "    img=cv2.resize(img,(224,224))\n",
        "    img=np.float32(img)\n",
        "    img/=255.0\n",
        "    img = img.reshape(224, 224, 3)\n",
        "    x.append(img)\n",
        "    y.append(2)"
      ],
      "metadata": {
        "id": "VWMICQ7IcITq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dgtest = glob.glob('/content/drive/MyDrive/Dataset/Disease Grading/Testing/*.jpg')\n",
        "\n",
        "ltest =glob.glob('/content/drive/MyDrive/Dataset/Localization/Testing/*.jpg')\n",
        "stest =glob.glob('/content/drive/MyDrive/Dataset/Segmentation/Testing/*.jpg')\n",
        "x_test=list()\n",
        "y_test=list()\n",
        "for i in dgtest:\n",
        "    img=cv2.imread(i,1)\n",
        "    img=cv2.resize(img,(224, 224))\n",
        "    img=np.float32(img)\n",
        "    img/=255.0\n",
        "    img = img.reshape(224, 224, 3)\n",
        "    x.append(img)\n",
        "    y.append(0)\n",
        "for j in ltest:\n",
        "    img=cv2.imread(j,1)\n",
        "    img=cv2.resize(img,(224, 224))\n",
        "    img=np.float32(img)\n",
        "    img/=255.0\n",
        "    img = img.reshape(224, 224, 3)\n",
        "    x.append(img)\n",
        "    y.append(1)\n",
        "for k in stest:\n",
        "    img=cv2.imread(k,1)\n",
        "    img=cv2.resize(img,(224, 224))\n",
        "    img=np.float32(img)\n",
        "    img/=255.0\n",
        "    img = img.reshape(224, 224, 3)\n",
        "    x.append(img)\n",
        "    y.append(2)"
      ],
      "metadata": {
        "id": "xYeBICi2cM3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define your dataset paths\n",
        "dgtrain = glob.glob('/content/drive/MyDrive/Dataset/Disease Grading/Training/*.jpg')\n",
        "ltrain = glob.glob('/content/drive/MyDrive/Dataset/Localization/Training/*.jpg')\n",
        "strain = glob.glob('/content/drive/MyDrive/Dataset/Segmentation/Training/*.jpg')\n",
        "\n",
        "dgtest = glob.glob('/content/drive/MyDrive/Dataset/Disease Grading/Testing/*.jpg')\n",
        "ltest = glob.glob('/content/drive/MyDrive/Dataset/Localization/Testing/*.jpg')\n",
        "stest = glob.glob('/content/drive/MyDrive/Dataset/Segmentation/Testing/*.jpg')\n",
        "\n",
        "# Function to load and preprocess images\n",
        "def load_and_preprocess_image(image_path):\n",
        "    img = cv2.imread(image_path, 1)\n",
        "    img = cv2.resize(img, (224, 224))\n",
        "    img = np.float32(img) / 255.0\n",
        "    img = img.reshape(224, 224, 3)\n",
        "    return img\n",
        "\n",
        "# Load and preprocess training data\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for i in dgtrain:\n",
        "    img = load_and_preprocess_image(i)\n",
        "    x.append(img)\n",
        "    y.append(0)\n",
        "\n",
        "for j in ltrain:\n",
        "    img = load_and_preprocess_image(j)\n",
        "    x.append(img)\n",
        "    y.append(1)\n",
        "\n",
        "for k in strain:\n",
        "    img = load_and_preprocess_image(k)\n",
        "    x.append(img)\n",
        "    y.append(2)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an ImageDataGenerator for augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Generate augmented images and append to the training set\n",
        "for i in range(len(x_train)):\n",
        "    img = x_train[i]\n",
        "    img = img.reshape((1,) + img.shape)  # Reshape to (1, height, width, channels) for flow method\n",
        "    for batch in datagen.flow(img, batch_size=1):\n",
        "        x_train.append(batch[0])\n",
        "        y_train.append(y_train[i])\n",
        "        break  # Exit the loop after one batch to avoid infinite loop\n",
        "\n",
        "# Similarly, you can perform data augmentation for validation data if needed\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "x_val = np.array(x_val)\n",
        "y_val = np.array(y_val)\n",
        "\n",
        "# Now, x_train and y_train contain augmented training data\n",
        "# x_val and y_val contain validation data\n",
        "# You can use these arrays to train your model\n"
      ],
      "metadata": {
        "id": "U_dXkYzJcRzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming you have loaded your data into the variables x and y correctly\n",
        "# x should contain the images, and y should contain the labels.\n",
        "\n",
        "# Convert data to NumPy arrays\n",
        "x_samp = np.asarray(x)\n",
        "y_samp = np.asarray(y)\n",
        "\n",
        "# Check the shapes of your data to ensure they match\n",
        "print(\"x_samp.shape:\", x_samp.shape)\n",
        "print(\"y_samp.shape:\", y_samp.shape)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_samp, y_samp, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check the shapes of the split datasets\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "#print(\"x_test shape:\", x_test.shape)\n",
        "#print(\"y_test shape:\", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3I35BW32cWR6",
        "outputId": "4bc039e9-a122-48d8-f50d-46d84d3d6a79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_samp.shape: (880, 224, 224, 3)\n",
            "y_samp.shape: (880,)\n",
            "x_train shape: (704, 224, 224, 3)\n",
            "y_train shape: (704,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYZpHokCcbVx",
        "outputId": "ffbcac25-a56a-41b8-84ed-78214720a644"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.applications import EfficientNetB7\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Assuming x_train, y_train, x_test, y_test are defined from the previous code\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "num_classes = 3  # Replace with the actual number of classes in your dataset\n",
        "y_train_one_hot = to_categorical(y_train, num_classes=3)\n",
        "y_test_one_hot = to_categorical(y_test, num_classes=3)\n",
        "\n",
        "# Load EfficientNetB7 pre-trained on ImageNet\n",
        "base_model = EfficientNetB7(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Build a model on top of the pre-trained base model\n",
        "model = Sequential()\n",
        "model.add(base_model)\n",
        "model.add(layers.GlobalAveragePooling2D())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Freeze the weights of the pre-trained base model\n",
        "base_model.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train_one_hot, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "evaluation = model.evaluate(x_test, y_test_one_hot, batch_size=32)\n",
        "print(\"Test Loss:\", evaluation[0])\n",
        "print(\"Test Accuracy:\", evaluation[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9FNRD55mHjZ",
        "outputId": "9603cc92-5da6-4858-b164-59938914c48f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb7_notop.h5\n",
            "258076736/258076736 [==============================] - 5s 0us/step\n",
            "Epoch 1/10\n",
            "18/18 [==============================] - 294s 15s/step - loss: 1.0537 - accuracy: 0.4529 - val_loss: 0.9402 - val_accuracy: 0.4894\n",
            "Epoch 2/10\n",
            "18/18 [==============================] - 282s 16s/step - loss: 0.9618 - accuracy: 0.4831 - val_loss: 0.8428 - val_accuracy: 0.4894\n",
            "Epoch 3/10\n",
            "18/18 [==============================] - 273s 15s/step - loss: 0.9378 - accuracy: 0.4600 - val_loss: 0.8454 - val_accuracy: 0.4894\n",
            "Epoch 4/10\n",
            "18/18 [==============================] - 273s 15s/step - loss: 0.9258 - accuracy: 0.4458 - val_loss: 0.8624 - val_accuracy: 0.4894\n",
            "Epoch 5/10\n",
            "18/18 [==============================] - 266s 15s/step - loss: 0.9422 - accuracy: 0.4583 - val_loss: 0.8473 - val_accuracy: 0.4681\n",
            "Epoch 6/10\n",
            "18/18 [==============================] - 268s 15s/step - loss: 0.9140 - accuracy: 0.4352 - val_loss: 0.8467 - val_accuracy: 0.4681\n",
            "Epoch 7/10\n",
            "18/18 [==============================] - 267s 15s/step - loss: 0.9257 - accuracy: 0.4707 - val_loss: 0.8413 - val_accuracy: 0.4681\n",
            "Epoch 8/10\n",
            "18/18 [==============================] - 267s 15s/step - loss: 0.9241 - accuracy: 0.4689 - val_loss: 0.8577 - val_accuracy: 0.4894\n",
            "Epoch 9/10\n",
            "18/18 [==============================] - 276s 15s/step - loss: 0.9139 - accuracy: 0.4050 - val_loss: 0.8464 - val_accuracy: 0.4894\n",
            "Epoch 10/10\n",
            "18/18 [==============================] - 263s 15s/step - loss: 0.9078 - accuracy: 0.4387 - val_loss: 0.8524 - val_accuracy: 0.4681\n",
            "6/6 [==============================] - 57s 9s/step - loss: 0.8757 - accuracy: 0.4886\n",
            "Test Loss: 0.8757311105728149\n",
            "Test Accuracy: 0.4886363744735718\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.applications import EfficientNetB7\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "import copy\n",
        "import random\n",
        "import csv\n",
        "\n",
        "# Assuming x and y are your data and labels\n",
        "x = np.array(x)\n",
        "y = np.array(y)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "NUM_OF_CLASSES = 3  # Replace with the actual number of classes in your dataset\n",
        "y_train_one_hot = to_categorical(y_train, num_classes=NUM_OF_CLASSES)\n",
        "y_test_one_hot = to_categorical(y_test, num_classes=NUM_OF_CLASSES)\n",
        "\n",
        "# Model configuration\n",
        "NUM_OF_CLIENTS = 3\n",
        "SELECT_CLIENTS = 0.5\n",
        "EPOCHS = 4\n",
        "CLIENT_EPOCHS = 5\n",
        "BATCH_SIZE = 10\n",
        "DROP_RATE = 0\n",
        "learning_rate = 0.0025\n",
        "\n",
        "# Define serverhist dictionary to store evaluation metrics\n",
        "serverhist = {\n",
        "    'loss': [],\n",
        "    'accuracy': [],\n",
        "    'f1': [],\n",
        "    'precision': [],\n",
        "    'recall': [],\n",
        "    'specificity': [],\n",
        "    'sensitivity': []\n",
        "}\n",
        "\n",
        "# Load EfficientNetB7 pre-trained on ImageNet\n",
        "base_model = EfficientNetB7(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Build a model on top of the pre-trained base model\n",
        "model = Sequential()\n",
        "model.add(base_model)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(NUM_OF_CLASSES, activation='softmax'))\n",
        "\n",
        "# Freeze the weights of the pre-trained base model\n",
        "base_model.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train_one_hot, epochs=10, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Split the data for each client\n",
        "x_train_clients = np.array_split(x_train, NUM_OF_CLIENTS)\n",
        "y_train_clients = np.array_split(y_train_one_hot, NUM_OF_CLIENTS)\n",
        "\n",
        "# Initialize avg_weight\n",
        "avg_weight = model.get_weights()\n",
        "\n",
        "# Federated Learning Loop\n",
        "for epoch in range(EPOCHS):\n",
        "    client_models = [Sequential() for _ in range(NUM_OF_CLIENTS)]\n",
        "\n",
        "    server_weight = []\n",
        "    selected_num = int(max(NUM_OF_CLIENTS * SELECT_CLIENTS, 1))\n",
        "    split_data_index = random.sample(range(NUM_OF_CLIENTS), selected_num)\n",
        "\n",
        "    for index in split_data_index:\n",
        "        client_models[index].add(base_model)\n",
        "        client_models[index].add(Flatten())\n",
        "        client_models[index].add(Dense(256, activation='relu'))\n",
        "        client_models[index].add(Dense(NUM_OF_CLASSES, activation='softmax'))\n",
        "\n",
        "        client_models[index].compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "        # Client update\n",
        "        client_models[index].set_weights(avg_weight)\n",
        "        client_models[index].fit(x_train_clients[index], y_train_clients[index],\n",
        "                                 epochs=CLIENT_EPOCHS,\n",
        "                                 batch_size=BATCH_SIZE,\n",
        "                                 verbose=1,\n",
        "                                 validation_split=0.2)\n",
        "\n",
        "        server_weight.append(copy.deepcopy(client_models[index].get_weights()))\n",
        "\n",
        "    avg_weight = np.mean(server_weight, axis=0)\n",
        "    model.set_weights(avg_weight)\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    evaluation = model.evaluate(x_test, y_test_one_hot, batch_size=32)\n",
        "\n",
        "    serverhist['accuracy'].append(evaluation[1])\n",
        "    serverhist['loss'].append(evaluation[0])\n",
        "\n",
        "# Print the results\n",
        "print(\"\\t\\t\\tFed Avg\")\n",
        "print(\"Round\\tAccuracy\\tLoss\")\n",
        "for i in range(EPOCHS):\n",
        "    print(\"{}\\t{}\\t{}\".format(\n",
        "        i + 1,\n",
        "        serverhist['accuracy'][i],\n",
        "        serverhist['loss'][i]\n",
        "    ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCkXLybjzBiq",
        "outputId": "0e23f5d8-12fb-4d18-f641-6990c16e5cb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "20/20 [==============================] - 283s 13s/step - loss: 80.3189 - accuracy: 0.4771 - val_loss: 1.0991 - val_accuracy: 0.4648\n",
            "Epoch 2/10\n",
            "20/20 [==============================] - 253s 13s/step - loss: 1.0946 - accuracy: 0.4645 - val_loss: 1.0867 - val_accuracy: 0.4648\n",
            "Epoch 3/10\n",
            "20/20 [==============================] - 254s 13s/step - loss: 1.3666 - accuracy: 0.4613 - val_loss: 1.0687 - val_accuracy: 0.4648\n",
            "Epoch 4/10\n",
            "20/20 [==============================] - 256s 13s/step - loss: 2.5918 - accuracy: 0.4376 - val_loss: 1.0509 - val_accuracy: 0.4648\n",
            "Epoch 5/10\n",
            "20/20 [==============================] - 257s 13s/step - loss: 1.0450 - accuracy: 0.4645 - val_loss: 1.0354 - val_accuracy: 0.4648\n",
            "Epoch 6/10\n",
            "20/20 [==============================] - 256s 13s/step - loss: 1.0307 - accuracy: 0.4645 - val_loss: 1.0215 - val_accuracy: 0.4648\n",
            "Epoch 7/10\n",
            "20/20 [==============================] - 257s 13s/step - loss: 1.0177 - accuracy: 0.4645 - val_loss: 1.0086 - val_accuracy: 0.4648\n",
            "Epoch 8/10\n",
            "20/20 [==============================] - 258s 13s/step - loss: 1.0058 - accuracy: 0.4645 - val_loss: 0.9966 - val_accuracy: 0.4648\n",
            "Epoch 9/10\n",
            "20/20 [==============================] - 255s 13s/step - loss: 0.9946 - accuracy: 0.4645 - val_loss: 0.9860 - val_accuracy: 0.4648\n",
            "Epoch 10/10\n",
            "20/20 [==============================] - 254s 13s/step - loss: 0.9847 - accuracy: 0.4645 - val_loss: 0.9761 - val_accuracy: 0.4648\n",
            "Epoch 1/5\n",
            "19/19 [==============================] - 107s 5s/step - loss: 0.9894 - accuracy: 0.4096 - val_loss: 0.9588 - val_accuracy: 0.5106\n",
            "Epoch 2/5\n",
            "19/19 [==============================] - 81s 4s/step - loss: 0.9818 - accuracy: 0.4734 - val_loss: 0.9495 - val_accuracy: 0.5106\n",
            "Epoch 3/5\n",
            "19/19 [==============================] - 80s 4s/step - loss: 0.9752 - accuracy: 0.4734 - val_loss: 0.9408 - val_accuracy: 0.5106\n",
            "Epoch 4/5\n",
            "19/19 [==============================] - 80s 4s/step - loss: 0.9696 - accuracy: 0.4734 - val_loss: 0.9329 - val_accuracy: 0.5106\n",
            "Epoch 5/5\n",
            "19/19 [==============================] - 81s 4s/step - loss: 0.9641 - accuracy: 0.4734 - val_loss: 0.9263 - val_accuracy: 0.5106\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  arr = asanyarray(a)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6/6 [==============================] - 55s 9s/step - loss: 0.9396 - accuracy: 0.4545\n",
            "Epoch 1/5\n",
            "19/19 [==============================] - 99s 4s/step - loss: 0.9220 - accuracy: 0.5160 - val_loss: 0.9732 - val_accuracy: 0.4894\n",
            "Epoch 2/5\n",
            "19/19 [==============================] - 80s 4s/step - loss: 0.9134 - accuracy: 0.5160 - val_loss: 0.9692 - val_accuracy: 0.4894\n",
            "Epoch 3/5\n",
            "19/19 [==============================] - 80s 4s/step - loss: 0.9061 - accuracy: 0.5160 - val_loss: 0.9658 - val_accuracy: 0.4894\n",
            "Epoch 4/5\n",
            "19/19 [==============================] - 80s 4s/step - loss: 0.8992 - accuracy: 0.5160 - val_loss: 0.9631 - val_accuracy: 0.4894\n",
            "Epoch 5/5\n",
            "19/19 [==============================] - 80s 4s/step - loss: 0.8938 - accuracy: 0.5160 - val_loss: 0.9608 - val_accuracy: 0.4894\n",
            "6/6 [==============================] - 54s 9s/step - loss: 0.9111 - accuracy: 0.4545\n",
            "Epoch 1/5\n",
            "19/19 [==============================] - 104s 4s/step - loss: 0.9399 - accuracy: 0.4734 - val_loss: 0.8885 - val_accuracy: 0.5106\n",
            "Epoch 2/5\n",
            "19/19 [==============================] - 78s 4s/step - loss: 0.9373 - accuracy: 0.4734 - val_loss: 0.8856 - val_accuracy: 0.5106\n",
            "Epoch 3/5\n",
            "19/19 [==============================] - 78s 4s/step - loss: 0.9361 - accuracy: 0.4734 - val_loss: 0.8827 - val_accuracy: 0.5106\n",
            "Epoch 4/5\n",
            "19/19 [==============================] - 78s 4s/step - loss: 0.9344 - accuracy: 0.4734 - val_loss: 0.8804 - val_accuracy: 0.5106\n",
            "Epoch 5/5\n",
            "19/19 [==============================] - 78s 4s/step - loss: 0.9518 - accuracy: 0.4628 - val_loss: 0.8785 - val_accuracy: 0.5106\n",
            "6/6 [==============================] - 56s 9s/step - loss: 0.9003 - accuracy: 0.4545\n",
            "Epoch 1/5\n",
            "19/19 [==============================] - 103s 4s/step - loss: 1.3522 - accuracy: 0.4118 - val_loss: 0.8701 - val_accuracy: 0.4894\n",
            "Epoch 2/5\n",
            "19/19 [==============================] - 85s 5s/step - loss: 0.9122 - accuracy: 0.4118 - val_loss: 0.8480 - val_accuracy: 0.4894\n",
            "Epoch 3/5\n",
            "19/19 [==============================] - 85s 5s/step - loss: 0.9034 - accuracy: 0.4545 - val_loss: 0.8447 - val_accuracy: 0.4894\n",
            "Epoch 4/5\n",
            "19/19 [==============================] - 88s 5s/step - loss: 0.9003 - accuracy: 0.5241 - val_loss: 0.8418 - val_accuracy: 0.4894\n",
            "Epoch 5/5\n",
            "19/19 [==============================] - 77s 4s/step - loss: 0.8976 - accuracy: 0.5241 - val_loss: 0.8393 - val_accuracy: 0.4894\n",
            "6/6 [==============================] - 59s 9s/step - loss: 0.8882 - accuracy: 0.4886\n",
            "\t\t\tFed Avg\n",
            "Round\tAccuracy\tLoss\n",
            "1\t0.4545454680919647\t0.9396350979804993\n",
            "2\t0.4545454680919647\t0.9111453890800476\n",
            "3\t0.4545454680919647\t0.9003192782402039\n",
            "4\t0.4886363744735718\t0.8881980776786804\n"
          ]
        }
      ]
    }
  ]
}