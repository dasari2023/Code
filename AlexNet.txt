import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import tensorflow
!pip install tensorflow

from google.colab import drive
drive.mount('/content/drive')

import os
#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import tensorflow as tf
from sklearn.preprocessing import OneHotEncoder
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.datasets import mnist
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.backend import image_data_format
from keras.models import Sequential
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from keras.models import Sequential
from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import confusion_matrix
from keras.layers import Dense
from keras.layers import Conv2D, DepthwiseConv2D, SeparableConv2D
from keras.layers import AvgPool2D, MaxPooling2D, Dropout, Flatten, BatchNormalization
#from keras.constraints import maxnorm
import matplotlib.pyplot as plt
import numpy as np
import copy
import random
import sys
import glob
import keras
import cv2
import csv
import time
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from keras.callbacks import ModelCheckpoint

pip install pytesseract

import cv2
import numpy as np
import pytesseract
from PIL import Image
from pytesseract import image_to_string


pip install opencv-python

x,y1, y = [], [],[]
#train_labels = {
   # "fire":0,"nofire":1
#}
x_train=list()
y_train=list()

dgtrain =glob.glob('/content/drive/MyDrive/Dataset/Disease Grading/Training/*.jpg')
ltrain =glob.glob('/content/drive/MyDrive/Dataset/Localization/Training/*.jpg')
strain =glob.glob('/content/drive/MyDrive/Dataset/Segmentation/Training/*.jpg')
for i in dgtrain:
    img=cv2.imread(i,1)
    img=cv2.resize(img,(224,224))
    img=np.float32(img)
    img/=255.0
    img = img.reshape(224, 224, 3)
    x.append(img)
    y.append(0)
for j in ltrain:
    img=cv2.imread(j,1)
    img=cv2.resize(img,(224,224))
    img=np.float32(img)
    img/=255.0
    img = img.reshape(224, 224, 3)
    x.append(img)
    y.append(1)
for k in strain:
    img=cv2.imread(k,1)
    img=cv2.resize(img,(224,224))
    img=np.float32(img)
    img/=255.0
    img = img.reshape(224, 224, 3)
    x.append(img)
    y.append(2)


dgtest = glob.glob('/content/drive/MyDrive/Dataset/Disease Grading/Testing/*.jpg')
ltest =glob.glob('/content/drive/MyDrive/Dataset/Localization/Testing/*.jpg')
stest =glob.glob('/content/drive/MyDrive/Dataset/Segmentation/Testing/*.jpg')
x_test=list()
y_test=list()
for i in dgtest:
    img=cv2.imread(i,1)
    img=cv2.resize(img,(224, 224))
    img=np.float32(img)
    img/=255.0
    img = img.reshape(224, 224, 3)
    x.append(img)
    y.append(0)
for j in ltest:
    img=cv2.imread(j,1)
    img=cv2.resize(img,(224, 224))
    img=np.float32(img)
    img/=255.0
    img = img.reshape(224, 224, 3)
    x.append(img)
    y.append(1)
for k in stest:
    img=cv2.imread(k,1)
    img=cv2.resize(img,(224, 224))
    img=np.float32(img)
    img/=255.0
    img = img.reshape(224, 224, 3)
    x.append(img)
    y.append(2)

import cv2
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Define your dataset paths
dgtrain = glob.glob('/content/drive/MyDrive/Dataset/Disease Grading/Training/*.jpg')
ltrain = glob.glob('/content/drive/MyDrive/Dataset/Localization/Training/*.jpg')
strain = glob.glob('/content/drive/MyDrive/Dataset/Segmentation/Training/*.jpg')

dgtest = glob.glob('/content/drive/MyDrive/Dataset/Disease Grading/Testing/*.jpg')
ltest = glob.glob('/content/drive/MyDrive/Dataset/Localization/Testing/*.jpg')
stest = glob.glob('/content/drive/MyDrive/Dataset/Segmentation/Testing/*.jpg')

# Function to load and preprocess images
def load_and_preprocess_image(image_path):
    img = cv2.imread(image_path, 1)
    img = cv2.resize(img, (224, 224))
    img = np.float32(img) / 255.0
    img = img.reshape(224, 224, 3)
    return img

# Load and preprocess training data
x = []
y = []

for i in dgtrain:
    img = load_and_preprocess_image(i)
    x.append(img)
    y.append(0)

for j in ltrain:
    img = load_and_preprocess_image(j)
    x.append(img)
    y.append(1)

for k in strain:
    img = load_and_preprocess_image(k)
    x.append(img)
    y.append(2)

# Split the data into training and validation sets
x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)

# Create an ImageDataGenerator for augmentation
datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

# Generate augmented images and append to the training set
for i in range(len(x_train)):
    img = x_train[i]
    img = img.reshape((1,) + img.shape)  # Reshape to (1, height, width, channels) for flow method
    for batch in datagen.flow(img, batch_size=1):
        x_train.append(batch[0])
        y_train.append(y_train[i])
        break  # Exit the loop after one batch to avoid infinite loop

# Similarly, you can perform data augmentation for validation data if needed

# Convert lists to numpy arrays
x_train = np.array(x_train)
y_train = np.array(y_train)

x_val = np.array(x_val)
y_val = np.array(y_val)

# Now, x_train and y_train contain augmented training data
# x_val and y_val contain validation data
# You can use these arrays to train your model


import numpy as np
from sklearn.model_selection import train_test_split

# Assuming you have loaded your data into the variables x and y correctly
# x should contain the images, and y should contain the labels.

# Convert data to NumPy arrays
x_samp = np.asarray(x)
y_samp = np.asarray(y)

# Check the shapes of your data to ensure they match
print("x_samp.shape:", x_samp.shape)
print("y_samp.shape:", y_samp.shape)

# Split the data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x_samp, y_samp, test_size=0.2, random_state=42)

# Check the shapes of the split datasets
print("x_train shape:", x_train.shape)
print("y_train shape:", y_train.shape)
#print("x_test shape:", x_test.shape)
#print("y_test shape:", y_test.shape)

pip install tensorflow scikit-learn


import keras
from keras.models import Sequential, load_model
from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D
from keras.layers import BatchNormalization
import numpy as np
from keras.utils import to_categorical
# Assuming y_train is a 1D array of class labels
y_train_one_hot = to_categorical(y_train, num_classes=3)


np.random.seed(1000)

# Create a Sequential model
AlexNet = Sequential()

# Add the 1st Convolutional Layer
AlexNet.add(Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11), strides=(4,4), padding='same'))
AlexNet.add(BatchNormalization())
AlexNet.add(Activation('relu'))
AlexNet.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='same'))

# Add the 2nd Convolutional Layer
AlexNet.add(Conv2D(filters=256, kernel_size=(5, 5), strides=(1,1), padding='same'))
AlexNet.add(BatchNormalization())
AlexNet.add(Activation('relu'))
AlexNet.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='same'))

# Add the 3rd Convolutional Layer
AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))
AlexNet.add(BatchNormalization())
AlexNet.add(Activation('relu'))

# Add the 4th Convolutional Layer
AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))
AlexNet.add(BatchNormalization())
AlexNet.add(Activation('relu'))

# Add the 5th Convolutional Layer
AlexNet.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same'))
AlexNet.add(BatchNormalization())
AlexNet.add(Activation('relu'))
AlexNet.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='same'))

# Flatten the input
AlexNet.add(Flatten())

# Add the 1st Fully Connected Layer
AlexNet.add(Dense(4096, input_shape=(32,32,3,)))
AlexNet.add(BatchNormalization())
AlexNet.add(Activation('relu'))
AlexNet.add(Dropout(0.4))

# Add the 2nd Fully Connected Layer
AlexNet.add(Dense(4096))
AlexNet.add(BatchNormalization())
AlexNet.add(Activation('relu'))
AlexNet.add(Dropout(0.4))

# Add the Output layer
AlexNet.add(Dense(3))
AlexNet.add(BatchNormalization())
AlexNet.add(Activation('softmax'))
AlexNet.summary()
# Print the model summary
# Compile the AlexNet model with categorical cross-entropy loss, SGD optimizer, and accuracy metric
AlexNet.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])

# Train the AlexNet model on the training data, for a total of 20 epochs, with validation data for monitoring performance
# X_train_gray_norm and y_train represent the preprocessed and normalized training data
# X_validation_gray_norm and y_validation represent the preprocessed and normalized validation data
#history = AlexNet.fit(x_train, y_train, epochs=10)
# Now, train your model using y_train_one_hot
history = AlexNet.fit(x_train, y_train_one_hot, epochs=10)


import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import tensorflow as tf
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.applications import DenseNet201
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split

import numpy as np
import copy
import random
import csv
x = np.array(x)
y=np.array(y)
# Client config
NUM_OF_CLIENTS = 3
SELECT_CLIENTS = 0.5
EPOCHS = 4
CLIENT_EPOCHS = 5
BATCH_SIZE = 10
DROP_RATE = 0

# Define serverhist dictionary to store evaluation metrics
serverhist = {
    'loss': [],
    'accuracy': [],
    'f1': [],
    'precision': [],
    'recall': [],
    'specificity': [],
    'sensitivity': []
}

# Corrected code
evaluation = [7]
result = evaluation[0]
#print(result)

# Model config
LOSS = 'categorical_crossentropy'
NUM_OF_CLASSES = 3
learning_rate = 0.0025

AlexNet = Sequential()

# Add the pre-trained DenseNet201 model as a base model
base_model = DenseNet201(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
AlexNet.add(base_model)


# Add the 1st Convolutional Layer
AlexNet.add(Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11), strides=(4,4), padding='same'))
AlexNet.add(BatchNormalization())
AlexNet.add(Activation('relu'))
AlexNet.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='same'))

# Add the 2nd Convolutional Layer
AlexNet.add(Conv2D(filters=256, kernel_size=(5, 5), strides=(1,1), padding='same'))
AlexNet.add(BatchNormalization())
AlexNet.add(Activation('relu'))
AlexNet.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='same'))

# Add the 3rd Convolutional Layer
AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))
AlexNet.add(BatchNormalization())
AlexNet.add(Activation('relu'))

# Add the 4th Convolutional Layer
AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))
AlexNet.add(BatchNormalization())
AlexNet.add(Activation('relu'))

# Add the 5th Convolutional Layer
AlexNet.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same'))
AlexNet.add(BatchNormalization())
AlexNet.add(Activation('relu'))
AlexNet.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='same'))

# Flatten the input
AlexNet.add(Flatten())

# Add the 1st Fully Connected Layer
AlexNet.add(Dense(4096, input_shape=(224,224,3,)))
AlexNet.add(BatchNormalization())
AlexNet.add(Activation('relu'))
AlexNet.add(Dropout(0.4))

# Add the 2nd Fully Connected Layer
AlexNet.add(Dense(4096))
AlexNet.add(BatchNormalization())
AlexNet.add(Activation('relu'))
AlexNet.add(Dropout(0.4))

# Add the Output layer
AlexNet.add(Dense(3))
AlexNet.add(BatchNormalization())
AlexNet.add(Activation('softmax'))
AlexNet.summary()

# Train the model
#model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.1)
# Split the data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)


# Split the data for each client
x_train_clients = np.array_split(x_train, NUM_OF_CLIENTS)
y_train_clients = np.array_split(y_train, NUM_OF_CLIENTS)
y_train_clients_one_hot = [to_categorical(labels) for labels in y_train_clients]


y_test_categorical = to_categorical(y_test)
# Use x_test directly for evaluation
y_test_one_hot = to_categorical(y_test, num_classes=3)

def init_model():
    fl_model = Sequential()
    fl_model.add(base_model)
    fl_model.add(Flatten())
    fl_model.add(Dense(256, activation='relu'))
    fl_model.add(Dense(NUM_OF_CLASSES, activation='softmax'))
    fl_model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])
    return fl_model

def client_update(index, client, now_epoch, avg_weight):
    #print("Client {}/{} fitting".format(index + 1, int(NUM_OF_CLIENTS * SELECT_CLIENTS)))
    client_model = client_models[index]
    if now_epoch != 0:
        client.set_weights(avg_weight)

    client.fit(x_train_clients[index], to_categorical(y_train_clients[index], num_classes=3),
               epochs=CLIENT_EPOCHS,
               batch_size=BATCH_SIZE,
               verbose=1,
               validation_split=0.2,
               )

    return client


# Define fedAVG function
def fedAVG(server_weight):
    avg_weight = np.array(server_weight[0])

    if len(server_weight) > 1:
        for i in range(1, len(server_weight)):
            avg_weight += server_weight[i]

        avg_weight = avg_weight / len(server_weight)

    return avg_weight

# Initialization
fl_model = init_model()
fl_model.summary()

# Split the data for each client
x_train_clients = np.array_split(x_train, NUM_OF_CLIENTS)
y_train_clients = np.array_split(y_train, NUM_OF_CLIENTS)
y_train_clients_one_hot = [to_categorical(labels) for labels in y_train_clients]

# Initialize avg_weight
avg_weight = fl_model.get_weights()

# Federated Learning Loop
for epoch in range(EPOCHS):
    client_models = [init_model() for _ in range(NUM_OF_CLIENTS)]

    server_weight = []
    selected_num = int(max(NUM_OF_CLIENTS * SELECT_CLIENTS, 1))
    split_data_index = random.sample(range(NUM_OF_CLIENTS), selected_num)

    for index in split_data_index:
        client_models[index] = client_update(index, client_models[index], epoch, avg_weight)
        server_weight.append(copy.deepcopy(client_models[index].get_weights()))

    avg_weight = fedAVG(server_weight)
    fl_model.set_weights(avg_weight)

    #print("Server {}/{} evaluate".format(epoch + 1, EPOCHS))
    # Use x_test directly for evaluation
    y_test_int = np.array(y_test)  # Convert to numpy array if not already
    evaluation = model.evaluate(x_test, y_test_categorical, batch_size=10, verbose=1)
    #print("Test loss:", evaluation[0])
    #print("Test accuracy:", evaluation[1])
    #print("Test f1:", evaluation[2])
    #print("Test precision:", evaluation[3])
    #print("Test recall:", evaluation[4])
    #print("Test specificity:", evaluation[5])
    #print("Test sensitivity:", evaluation[6])
    serverhist['accuracy'].append(evaluation[0])
    serverhist['loss'].append(evaluation[0])
    serverhist['f1'].append(evaluation[0])
    serverhist['precision'].append(evaluation[0])
    serverhist['recall'].append(evaluation[0])
    serverhist['specificity'].append(evaluation[0])
    serverhist['sensitivity'].append(evaluation[0])


# Print the results
print("\t\t\tFed Avg")
print("Round\tAccuracy\tLoss")
for i in range(EPOCHS):
    print("{}\t{}\t{}".format(
        i + 1,
        serverhist['accuracy'][i],
        serverhist['loss'][i],
        serverhist['f1'][i],
        serverhist['precision'][i],
        serverhist['recall'][i],
        serverhist['specificity'][i],
        serverhist['sensitivity'][i]
    ))