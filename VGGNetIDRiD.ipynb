{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1FpcOFPySqbtzl_Vy_zRpPk5dnJtECDQh",
      "authorship_tag": "ABX9TyOMLIOETi9jYQCwdY1lAFUY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dasari2023/Code/blob/main/VGGNetIDRiD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MICyeCi-KxvZ",
        "outputId": "5cafed04-2768-4e75-ac51-9b5edc1b185e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.2)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "import tensorflow\n",
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5M2omfFK6sw",
        "outputId": "109e565b-ed74-48b5-9ed6-22aafccfbfd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.backend import image_data_format\n",
        "from keras.models import Sequential\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from keras.models import Sequential\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Conv2D, DepthwiseConv2D, SeparableConv2D\n",
        "from keras.layers import AvgPool2D, MaxPooling2D, Dropout, Flatten, BatchNormalization\n",
        "#from keras.constraints import maxnorm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import copy\n",
        "import random\n",
        "import sys\n",
        "import glob\n",
        "import keras\n",
        "import cv2\n",
        "import csv\n",
        "import time\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import ModelCheckpoint\n"
      ],
      "metadata": {
        "id": "cretZXgXK9FZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pytesseract\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsfBtcXpLBu5",
        "outputId": "c71c18b4-582b-4bc9-ace1-bec99b02d772"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "from pytesseract import image_to_string"
      ],
      "metadata": {
        "id": "ZxUBL2dxLEua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install opencv-python\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQQXuVuvLH0p",
        "outputId": "c5231688-c2a2-4272-912f-1df2994ab042"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x,y1, y = [], [],[]\n",
        "#train_labels = {\n",
        "   # \"fire\":0,\"nofire\":1\n",
        "#}\n",
        "x_train=list()\n",
        "y_train=list()\n",
        "#fire = glob.glob('/content/sample_data/Forest Fire Dataset/Training/fire/*.jpg')\n",
        "#nfire =glob.glob('/content/sample_data/Forest Fire Dataset/Training/nofire/*.jpg')\n",
        "dgtrain =glob.glob('/content/drive/MyDrive/Dataset/Disease Grading/Training/*.jpg')\n",
        "ltrain =glob.glob('/content/drive/MyDrive/Dataset/Localization/Training/*.jpg')\n",
        "strain =glob.glob('/content/drive/MyDrive/Dataset/Segmentation/Training/*.jpg')\n",
        "for i in dgtrain:\n",
        "    img=cv2.imread(i,1)\n",
        "    img=cv2.resize(img,(224,224))\n",
        "    img=np.float32(img)\n",
        "    img/=255.0\n",
        "    img = img.reshape(224, 224, 3)\n",
        "    x.append(img)\n",
        "    y.append(0)\n",
        "for j in ltrain:\n",
        "    img=cv2.imread(j,1)\n",
        "    img=cv2.resize(img,(224,224))\n",
        "    img=np.float32(img)\n",
        "    img/=255.0\n",
        "    img = img.reshape(224, 224, 3)\n",
        "    x.append(img)\n",
        "    y.append(1)\n",
        "for k in strain:\n",
        "    img=cv2.imread(k,1)\n",
        "    img=cv2.resize(img,(224,224))\n",
        "    img=np.float32(img)\n",
        "    img/=255.0\n",
        "    img = img.reshape(224, 224, 3)\n",
        "    x.append(img)\n",
        "    y.append(2)"
      ],
      "metadata": {
        "id": "5eg1o5NiLOVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#firel = glob.glob('/content/sample_data/Forest Fire Dataset/Testing/fire/*.jpg')\n",
        "#nfirel=glob.glob('/content/sample_data/Forest Fire Dataset/Testing/nofire/*.jpg')\n",
        "dgtest = glob.glob('/content/drive/MyDrive/Dataset/Disease Grading/Testing/*.jpg')\n",
        "#dgtest =glob.glob('/content/drive/MyDrive/Dataset/Disease Grading/Training/*.jpg')\n",
        "ltest =glob.glob('/content/drive/MyDrive/Dataset/Localization/Testing/*.jpg')\n",
        "stest =glob.glob('/content/drive/MyDrive/Dataset/Segmentation/Testing/*.jpg')\n",
        "x_test=list()\n",
        "y_test=list()\n",
        "for i in dgtest:\n",
        "    img=cv2.imread(i,1)\n",
        "    img=cv2.resize(img,(224, 224))\n",
        "    img=np.float32(img)\n",
        "    img/=255.0\n",
        "    img = img.reshape(224, 224, 3)\n",
        "    x.append(img)\n",
        "    y.append(0)\n",
        "for j in ltest:\n",
        "    img=cv2.imread(j,1)\n",
        "    img=cv2.resize(img,(224, 224))\n",
        "    img=np.float32(img)\n",
        "    img/=255.0\n",
        "    img = img.reshape(224, 224, 3)\n",
        "    x.append(img)\n",
        "    y.append(1)\n",
        "for k in stest:\n",
        "    img=cv2.imread(k,1)\n",
        "    img=cv2.resize(img,(224, 224))\n",
        "    img=np.float32(img)\n",
        "    img/=255.0\n",
        "    img = img.reshape(224, 224, 3)\n",
        "    x.append(img)\n",
        "    y.append(2)"
      ],
      "metadata": {
        "id": "OFRUOkdULSwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define your dataset paths\n",
        "dgtrain = glob.glob('/content/drive/MyDrive/Dataset/Disease Grading/Training/*.jpg')\n",
        "ltrain = glob.glob('/content/drive/MyDrive/Dataset/Localization/Training/*.jpg')\n",
        "strain = glob.glob('/content/drive/MyDrive/Dataset/Segmentation/Training/*.jpg')\n",
        "\n",
        "dgtest = glob.glob('/content/drive/MyDrive/Dataset/Disease Grading/Testing/*.jpg')\n",
        "ltest = glob.glob('/content/drive/MyDrive/Dataset/Localization/Testing/*.jpg')\n",
        "stest = glob.glob('/content/drive/MyDrive/Dataset/Segmentation/Testing/*.jpg')\n",
        "\n",
        "# Function to load and preprocess images\n",
        "def load_and_preprocess_image(image_path):\n",
        "    img = cv2.imread(image_path, 1)\n",
        "    img = cv2.resize(img, (224, 224))\n",
        "    img = np.float32(img) / 255.0\n",
        "    img = img.reshape(224, 224, 3)\n",
        "    return img\n",
        "\n",
        "# Load and preprocess training data\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for i in dgtrain:\n",
        "    img = load_and_preprocess_image(i)\n",
        "    x.append(img)\n",
        "    y.append(0)\n",
        "\n",
        "for j in ltrain:\n",
        "    img = load_and_preprocess_image(j)\n",
        "    x.append(img)\n",
        "    y.append(1)\n",
        "\n",
        "for k in strain:\n",
        "    img = load_and_preprocess_image(k)\n",
        "    x.append(img)\n",
        "    y.append(2)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an ImageDataGenerator for augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Generate augmented images and append to the training set\n",
        "for i in range(len(x_train)):\n",
        "    img = x_train[i]\n",
        "    img = img.reshape((1,) + img.shape)  # Reshape to (1, height, width, channels) for flow method\n",
        "    for batch in datagen.flow(img, batch_size=1):\n",
        "        x_train.append(batch[0])\n",
        "        y_train.append(y_train[i])\n",
        "        break  # Exit the loop after one batch to avoid infinite loop\n",
        "\n",
        "# Similarly, you can perform data augmentation for validation data if needed\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "x_val = np.array(x_val)\n",
        "y_val = np.array(y_val)\n",
        "\n",
        "# Now, x_train and y_train contain augmented training data\n",
        "# x_val and y_val contain validation data\n",
        "# You can use these arrays to train your model\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Qdv07wZoLZej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming you have loaded your data into the variables x and y correctly\n",
        "# x should contain the images, and y should contain the labels.\n",
        "\n",
        "# Convert data to NumPy arrays\n",
        "x_samp = np.asarray(x)\n",
        "y_samp = np.asarray(y)\n",
        "\n",
        "# Check the shapes of your data to ensure they match\n",
        "print(\"x_samp.shape:\", x_samp.shape)\n",
        "print(\"y_samp.shape:\", y_samp.shape)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_samp, y_samp, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check the shapes of the split datasets\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "#print(\"x_test shape:\", x_test.shape)\n",
        "#print(\"y_test shape:\", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4rAsM-iLmiX",
        "outputId": "297d601d-6c06-42a9-e4d6-59174879461c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_samp.shape: (880, 224, 224, 3)\n",
            "y_samp.shape: (880,)\n",
            "x_train shape: (704, 224, 224, 3)\n",
            "y_train shape: (704,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69brQH_PLalX",
        "outputId": "b18a28e0-9df4-4ad1-a940-dd90f00819ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Assuming x and y are your data and labels\n",
        "x_samp = np.asarray(x)\n",
        "y_samp = np.asarray(y)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_samp, y_samp, test_size=0.2, random_state=42)\n",
        "\n",
        "# Use one-hot encoding for the labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# VGGNet model with pre-trained weights (you can modify this based on your needs)\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the convolutional layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add the VGGNet base model\n",
        "model.add(base_model)\n",
        "\n",
        "# Flatten the output of VGGNet\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "# Add some dense layers for classification\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))  # Assuming binary classification, adjust for your needs\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Data augmentation to improve generalization\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Train the model with data augmentation\n",
        "history = model.fit(\n",
        "    datagen.flow(x_train, y_train_encoded, batch_size=32),\n",
        "    steps_per_epoch=len(x_train) // 32,\n",
        "    epochs=10,\n",
        "    validation_data=(x_test, y_test_encoded),\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y73DST7UNZYy",
        "outputId": "79d2d9aa-1074-4b01-d069-1d53b811b7fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 1s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "22/22 [==============================] - 631s 29s/step - loss: 4.6420 - accuracy: 0.4517 - val_loss: 2.0655 - val_accuracy: 0.4886\n",
            "Epoch 2/10\n",
            "22/22 [==============================] - 624s 29s/step - loss: 1.8105 - accuracy: 0.4688 - val_loss: 0.8523 - val_accuracy: 0.4886\n",
            "Epoch 3/10\n",
            "22/22 [==============================] - 625s 29s/step - loss: 0.9544 - accuracy: 0.4659 - val_loss: 0.7032 - val_accuracy: 0.4886\n",
            "Epoch 4/10\n",
            "22/22 [==============================] - 620s 28s/step - loss: 0.7165 - accuracy: 0.4602 - val_loss: 0.6736 - val_accuracy: 0.4886\n",
            "Epoch 5/10\n",
            "22/22 [==============================] - 618s 28s/step - loss: 0.6908 - accuracy: 0.4631 - val_loss: 0.6769 - val_accuracy: 0.4886\n",
            "Epoch 6/10\n",
            "22/22 [==============================] - 618s 28s/step - loss: 0.6775 - accuracy: 0.4616 - val_loss: 0.6744 - val_accuracy: 0.4886\n",
            "Epoch 7/10\n",
            "22/22 [==============================] - 599s 27s/step - loss: 0.6830 - accuracy: 0.4688 - val_loss: 0.6751 - val_accuracy: 0.4886\n",
            "Epoch 8/10\n",
            "22/22 [==============================] - 625s 29s/step - loss: 0.6787 - accuracy: 0.4659 - val_loss: 0.6750 - val_accuracy: 0.4886\n",
            "Epoch 9/10\n",
            "22/22 [==============================] - 623s 29s/step - loss: 0.6828 - accuracy: 0.4645 - val_loss: 0.6742 - val_accuracy: 0.4886\n",
            "Epoch 10/10\n",
            "22/22 [==============================] - 624s 29s/step - loss: 0.6774 - accuracy: 0.4616 - val_loss: 0.6717 - val_accuracy: 0.4886\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.applications import DenseNet201\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "import numpy as np\n",
        "import copy\n",
        "import random\n",
        "import csv\n",
        "x = np.array(x)\n",
        "y=np.array(y)\n",
        "# Client config\n",
        "NUM_OF_CLIENTS = 3\n",
        "SELECT_CLIENTS = 0.5\n",
        "EPOCHS = 4\n",
        "CLIENT_EPOCHS = 5\n",
        "BATCH_SIZE = 10\n",
        "DROP_RATE = 0\n",
        "\n",
        "# Define serverhist dictionary to store evaluation metrics\n",
        "serverhist = {\n",
        "    'loss': [],\n",
        "    'accuracy': [],\n",
        "    'f1': [],\n",
        "    'precision': [],\n",
        "    'recall': [],\n",
        "    'specificity': [],\n",
        "    'sensitivity': []\n",
        "}\n",
        "\n",
        "# Corrected code\n",
        "evaluation = [7]\n",
        "result = evaluation[0]\n",
        "#print(result)\n",
        "\n",
        "# Model config\n",
        "LOSS = 'categorical_crossentropy'\n",
        "NUM_OF_CLASSES = 3\n",
        "learning_rate = 0.0025\n",
        "\n",
        "# VGGNet model with pre-trained weights (you can modify this based on your needs)\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the convolutional layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add the VGGNet base model\n",
        "model.add(base_model)\n",
        "\n",
        "# Flatten the output of VGGNet\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "# Add some dense layers for classification\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))  # Assuming binary classification, adjust for your needs\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
        "# Split the data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Split the data for each client\n",
        "x_train_clients = np.array_split(x_train, NUM_OF_CLIENTS)\n",
        "y_train_clients = np.array_split(y_train, NUM_OF_CLIENTS)\n",
        "y_train_clients_one_hot = [to_categorical(labels) for labels in y_train_clients]\n",
        "\n",
        "\n",
        "y_test_categorical = to_categorical(y_test)\n",
        "# Use x_test directly for evaluation\n",
        "y_test_one_hot = to_categorical(y_test, num_classes=3)\n",
        "\n",
        "def init_model():\n",
        "    fl_model = Sequential()\n",
        "    fl_model.add(base_model)\n",
        "    fl_model.add(Flatten())\n",
        "    fl_model.add(Dense(256, activation='relu'))\n",
        "    fl_model.add(Dense(NUM_OF_CLASSES, activation='softmax'))\n",
        "    fl_model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return fl_model\n",
        "\n",
        "def client_update(index, client, now_epoch, avg_weight):\n",
        "    #print(\"Client {}/{} fitting\".format(index + 1, int(NUM_OF_CLIENTS * SELECT_CLIENTS)))\n",
        "    client_model = client_models[index]\n",
        "    if now_epoch != 0:\n",
        "        client.set_weights(avg_weight)\n",
        "\n",
        "    client.fit(x_train_clients[index], to_categorical(y_train_clients[index], num_classes=3),\n",
        "               epochs=CLIENT_EPOCHS,\n",
        "               batch_size=BATCH_SIZE,\n",
        "               verbose=1,\n",
        "               validation_split=0.2,\n",
        "               )\n",
        "\n",
        "    return client\n",
        "\n",
        "\n",
        "# Define fedAVG function\n",
        "def fedAVG(server_weight):\n",
        "    avg_weight = np.array(server_weight[0])\n",
        "\n",
        "    if len(server_weight) > 1:\n",
        "        for i in range(1, len(server_weight)):\n",
        "            avg_weight += server_weight[i]\n",
        "\n",
        "        avg_weight = avg_weight / len(server_weight)\n",
        "\n",
        "    return avg_weight\n",
        "\n",
        "# Initialization\n",
        "fl_model = init_model()\n",
        "fl_model.summary()\n",
        "\n",
        "# Split the data for each client\n",
        "x_train_clients = np.array_split(x_train, NUM_OF_CLIENTS)\n",
        "y_train_clients = np.array_split(y_train, NUM_OF_CLIENTS)\n",
        "y_train_clients_one_hot = [to_categorical(labels) for labels in y_train_clients]\n",
        "\n",
        "# Initialize avg_weight\n",
        "avg_weight = fl_model.get_weights()\n",
        "\n",
        "# Federated Learning Loop\n",
        "for epoch in range(EPOCHS):\n",
        "    client_models = [init_model() for _ in range(NUM_OF_CLIENTS)]\n",
        "\n",
        "    server_weight = []\n",
        "    selected_num = int(max(NUM_OF_CLIENTS * SELECT_CLIENTS, 1))\n",
        "    split_data_index = random.sample(range(NUM_OF_CLIENTS), selected_num)\n",
        "\n",
        "    for index in split_data_index:\n",
        "        client_models[index] = client_update(index, client_models[index], epoch, avg_weight)\n",
        "        server_weight.append(copy.deepcopy(client_models[index].get_weights()))\n",
        "\n",
        "    avg_weight = fedAVG(server_weight)\n",
        "    fl_model.set_weights(avg_weight)\n",
        "\n",
        "    #print(\"Server {}/{} evaluate\".format(epoch + 1, EPOCHS))\n",
        "    # Use x_test directly for evaluation\n",
        "    y_test_int = np.array(y_test)  # Convert to numpy array if not already\n",
        "    evaluation = model.evaluate(x_test, y_test_categorical, batch_size=10, verbose=1)\n",
        "    #print(\"Test loss:\", evaluation[0])\n",
        "    #print(\"Test accuracy:\", evaluation[1])\n",
        "    #print(\"Test f1:\", evaluation[2])\n",
        "    #print(\"Test precision:\", evaluation[3])\n",
        "    #print(\"Test recall:\", evaluation[4])\n",
        "    #print(\"Test specificity:\", evaluation[5])\n",
        "    #print(\"Test sensitivity:\", evaluation[6])\n",
        "    serverhist['accuracy'].append(evaluation[0])\n",
        "    serverhist['loss'].append(evaluation[0])\n",
        "    serverhist['f1'].append(evaluation[0])\n",
        "    serverhist['precision'].append(evaluation[0])\n",
        "    serverhist['recall'].append(evaluation[0])\n",
        "    serverhist['specificity'].append(evaluation[0])\n",
        "    serverhist['sensitivity'].append(evaluation[0])\n",
        "\n",
        "\n",
        "# Print the results\n",
        "print(\"\\t\\t\\tFed Avg\")\n",
        "print(\"Round\\tAccuracy\\tLoss\\tf1\\tprecision\\trecall\\tspecificity\\tsensitivity\")\n",
        "for i in range(EPOCHS):\n",
        "    print(\"{}\\t{}\\t{}\".format(\n",
        "        i + 1,\n",
        "        serverhist['accuracy'][i],\n",
        "        serverhist['loss'][i],\n",
        "        serverhist['f1'][i],\n",
        "        serverhist['precision'][i],\n",
        "        serverhist['recall'][i],\n",
        "        serverhist['specificity'][i],\n",
        "        serverhist['sensitivity'][i]\n",
        "    ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vkpUOgtFN-ay",
        "outputId": "fe1f9a30-d8d9-4716-8c2e-b6c0c4d3b2f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1260: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20/20 [==============================] - 522s 26s/step - loss: 0.0000e+00 - accuracy: 0.4724 - val_loss: 0.0000e+00 - val_accuracy: 0.4789\n",
            "Epoch 2/10\n",
            "20/20 [==============================] - 513s 26s/step - loss: 0.0000e+00 - accuracy: 0.4724 - val_loss: 0.0000e+00 - val_accuracy: 0.4789\n",
            "Epoch 3/10\n",
            "20/20 [==============================] - 503s 25s/step - loss: 0.0000e+00 - accuracy: 0.4724 - val_loss: 0.0000e+00 - val_accuracy: 0.4789\n",
            "Epoch 4/10\n",
            "20/20 [==============================] - 502s 25s/step - loss: 0.0000e+00 - accuracy: 0.4724 - val_loss: 0.0000e+00 - val_accuracy: 0.4789\n",
            "Epoch 5/10\n",
            "20/20 [==============================] - 505s 25s/step - loss: 0.0000e+00 - accuracy: 0.4724 - val_loss: 0.0000e+00 - val_accuracy: 0.4789\n",
            "Epoch 6/10\n",
            "20/20 [==============================] - 504s 25s/step - loss: 0.0000e+00 - accuracy: 0.4724 - val_loss: 0.0000e+00 - val_accuracy: 0.4789\n",
            "Epoch 7/10\n",
            "20/20 [==============================] - 467s 23s/step - loss: 0.0000e+00 - accuracy: 0.4724 - val_loss: 0.0000e+00 - val_accuracy: 0.4789\n",
            "Epoch 8/10\n",
            "20/20 [==============================] - 498s 25s/step - loss: 0.0000e+00 - accuracy: 0.4724 - val_loss: 0.0000e+00 - val_accuracy: 0.4789\n",
            "Epoch 9/10\n",
            "20/20 [==============================] - 504s 25s/step - loss: 0.0000e+00 - accuracy: 0.4724 - val_loss: 0.0000e+00 - val_accuracy: 0.4789\n",
            "Epoch 10/10\n",
            "20/20 [==============================] - 505s 25s/step - loss: 0.0000e+00 - accuracy: 0.4724 - val_loss: 0.0000e+00 - val_accuracy: 0.4789\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 25088)             0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 256)               6422784   \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 3)                 771       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 21138243 (80.64 MB)\n",
            "Trainable params: 6423555 (24.50 MB)\n",
            "Non-trainable params: 14714688 (56.13 MB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "19/19 [==============================] - 167s 9s/step - loss: 11.7516 - accuracy: 0.4202 - val_loss: 7.2913 - val_accuracy: 0.4894\n",
            "Epoch 2/5\n",
            "19/19 [==============================] - 156s 8s/step - loss: 3.2395 - accuracy: 0.4628 - val_loss: 2.9454 - val_accuracy: 0.5106\n",
            "Epoch 3/5\n",
            "19/19 [==============================] - 156s 8s/step - loss: 1.9840 - accuracy: 0.5213 - val_loss: 2.1068 - val_accuracy: 0.3617\n",
            "Epoch 4/5\n",
            "19/19 [==============================] - 166s 9s/step - loss: 2.8159 - accuracy: 0.5532 - val_loss: 3.8949 - val_accuracy: 0.4894\n",
            "Epoch 5/5\n",
            "19/19 [==============================] - 165s 9s/step - loss: 2.5805 - accuracy: 0.4840 - val_loss: 4.1848 - val_accuracy: 0.3617\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-868fb5efe013>:116: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  avg_weight = np.array(server_weight[0])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-868fb5efe013>\u001b[0m in \u001b[0;36m<cell line: 139>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;31m# Use x_test directly for evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0my_test_int\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Convert to numpy array if not already\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0mevaluation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_categorical\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0;31m#print(\"Test loss:\", evaluation[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;31m#print(\"Test accuracy:\", evaluation[1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__test_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2042, in test_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2025, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2013, in run_step  **\n        outputs = model.test_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1895, in test_step\n        self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1185, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5575, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 3) and (None, 1) are incompatible\n"
          ]
        }
      ]
    }
  ]
}