import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import tensorflow
!pip install tensorflow

from google.colab import drive
drive.mount('/content/drive')

import os
#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import tensorflow as tf
from sklearn.preprocessing import OneHotEncoder
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.datasets import mnist
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.backend import image_data_format
from keras.models import Sequential
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from keras.models import Sequential
from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import confusion_matrix
from keras.layers import Dense
from keras.layers import Conv2D, DepthwiseConv2D, SeparableConv2D
from keras.layers import AvgPool2D, MaxPooling2D, Dropout, Flatten, BatchNormalization
from keras.preprocessing.image import ImageDataGenerator
from keras.preprocessing import image
import matplotlib.pyplot as plt
import numpy as np
import copy
import random
import sys
import glob
import keras
import cv2
import csv
import time
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from keras.callbacks import ModelCheckpoint

pip install pytesseract

import cv2
import numpy as np
import pytesseract
from PIL import Image
from pytesseract import image_to_string



pip install opencv-python

x,y1, y = [], [],[]

x_train=list()
y_train=list()

dgtrain =glob.glob('/content/drive/MyDrive/Dataset/Disease Grading/Training/*.jpg')
ltrain =glob.glob('/content/drive/MyDrive/Dataset/Localization/Training/*.jpg')
strain =glob.glob('/content/drive/MyDrive/Dataset/Segmentation/Training/*.jpg')
for i in dgtrain:
    img=cv2.imread(i,1)
    img=cv2.resize(img,(224,224))
    img=np.float32(img)
    img/=255.0
    img = img.reshape(224, 224, 3)
    x.append(img)
    y.append(0)
for j in ltrain:
    img=cv2.imread(j,1)
    img=cv2.resize(img,(224,224))
    img=np.float32(img)
    img/=255.0
    img = img.reshape(224, 224, 3)
    x.append(img)
    y.append(1)
for k in strain:
    img=cv2.imread(k,1)
    img=cv2.resize(img,(224,224))
    img=np.float32(img)
    img/=255.0
    img = img.reshape(224, 224, 3)
    x.append(img)
    y.append(2)



dgtest = glob.glob('/content/drive/MyDrive/Dataset/Disease Grading/Testing/*.jpg')
ltest =glob.glob('/content/drive/MyDrive/Dataset/Localization/Testing/*.jpg')
stest =glob.glob('/content/drive/MyDrive/Dataset/Segmentation/Testing/*.jpg')
x_test=list()
y_test=list()
for i in dgtest:
    img=cv2.imread(i,1)
    img=cv2.resize(img,(224, 224))
    img=np.float32(img)
    img/=255.0
    img = img.reshape(224, 224, 3)
    x.append(img)
    y.append(0)
for j in ltest:
    img=cv2.imread(j,1)
    img=cv2.resize(img,(224, 224))
    img=np.float32(img)
    img/=255.0
    img = img.reshape(224, 224, 3)
    x.append(img)
    y.append(1)
for k in stest:
    img=cv2.imread(k,1)
    img=cv2.resize(img,(224, 224))
    img=np.float32(img)
    img/=255.0
    img = img.reshape(224, 224, 3)
    x.append(img)
    y.append(2)

import cv2
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Define your dataset paths
dgtrain = glob.glob('/content/drive/MyDrive/Dataset/Disease Grading/Training/*.jpg')
ltrain = glob.glob('/content/drive/MyDrive/Dataset/Localization/Training/*.jpg')
strain = glob.glob('/content/drive/MyDrive/Dataset/Segmentation/Training/*.jpg')

dgtest = glob.glob('/content/drive/MyDrive/Dataset/Disease Grading/Testing/*.jpg')
ltest = glob.glob('/content/drive/MyDrive/Dataset/Localization/Testing/*.jpg')
stest = glob.glob('/content/drive/MyDrive/Dataset/Segmentation/Testing/*.jpg')

# Function to load and preprocess images
def load_and_preprocess_image(image_path):
    img = cv2.imread(image_path, 1)
    img = cv2.resize(img, (224, 224))
    img = np.float32(img) / 255.0
    img = img.reshape(224, 224, 3)
    return img

# Load and preprocess training data
x = []
y = []

for i in dgtrain:
    img = load_and_preprocess_image(i)
    x.append(img)
    y.append(0)

for j in ltrain:
    img = load_and_preprocess_image(j)
    x.append(img)
    y.append(1)

for k in strain:
    img = load_and_preprocess_image(k)
    x.append(img)
    y.append(2)

# Split the data into training and validation sets
x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)

# Create an ImageDataGenerator for augmentation
datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

# Generate augmented images and append to the training set
for i in range(len(x_train)):
    img = x_train[i]
    img = img.reshape((1,) + img.shape)  # Reshape to (1, height, width, channels) for flow method
    for batch in datagen.flow(img, batch_size=1):
        x_train.append(batch[0])
        y_train.append(y_train[i])
        break  # Exit the loop after one batch to avoid infinite loop

# Similarly, you can perform data augmentation for validation data if needed

# Convert lists to numpy arrays
x_train = np.array(x_train)
y_train = np.array(y_train)

x_val = np.array(x_val)
y_val = np.array(y_val)

# Now, x_train and y_train contain augmented training data
# x_val and y_val contain validation data
# You can use these arrays to train your model


import numpy as np
from sklearn.model_selection import train_test_split

# Assuming you have loaded your data into the variables x and y correctly
# x should contain the images, and y should contain the labels.

# Convert data to NumPy arrays
x_samp = np.asarray(x)
y_samp = np.asarray(y)

# Check the shapes of your data to ensure they match
print("x_samp.shape:", x_samp.shape)
print("y_samp.shape:", y_samp.shape)

# Split the data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x_samp, y_samp, test_size=0.2, random_state=42)

# Check the shapes of the split datasets
print("x_train shape:", x_train.shape)
print("y_train shape:", y_train.shape)
print("x_test shape:", x_test.shape)
print("y_test shape:", y_test.shape)

pip install keras scikit-learn

from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense
from tensorflow.keras.models import Sequential

# Create a sequential model
VGGNet = Sequential()

# Add the first convolutional layer with 64 filters, kernel size of 3x3, padding "same", and ReLU activation
VGGNet.add(Conv2D(input_shape=(150,150,3), filters=96, kernel_size=(11, 11), padding="same", activation="relu"))

# Add the second convolutional layer with 64 filters, kernel size of 3x3, padding "same", and ReLU activation
VGGNet.add(Conv2D(filters=256, kernel_size=(5, 5), padding="same", activation="relu"))

# Add the first max pooling layer with pool size 2x2 and stride 2x2
VGGNet.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))

# Continue adding more convolutional and max pooling layers with increasing number of filters
VGGNet.add(Conv2D(filters=384, kernel_size=(3, 3), padding="same", activation="relu"))
VGGNet.add(Conv2D(filters=128, kernel_size=(3, 3), padding="same", activation="relu"))
VGGNet.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))

VGGNet.add(Conv2D(filters=256, kernel_size=(3, 3), padding="same", activation="relu"))
VGGNet.add(Conv2D(filters=256, kernel_size=(3, 3), padding="same", activation="relu"))
VGGNet.add(Conv2D(filters=256, kernel_size=(3, 3), padding="same", activation="relu"))
VGGNet.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))

VGGNet.add(Conv2D(filters=512, kernel_size=(3, 3), padding="same", activation="relu"))
VGGNet.add(Conv2D(filters=512, kernel_size=(3, 3), padding="same", activation="relu"))
VGGNet.add(Conv2D(filters=512, kernel_size=(3, 3), padding="same", activation="relu"))
VGGNet.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))

VGGNet.add(Conv2D(filters=512, kernel_size=(3, 3), padding="same", activation="relu"))
VGGNet.add(Conv2D(filters=512, kernel_size=(3, 3), padding="same", activation="relu"))
VGGNet.add(Conv2D(filters=512, kernel_size=(3, 3), padding="same", activation="relu"))
VGGNet.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))

# Flatten the output from the previous layers
VGGNet.add(Flatten())

# Add two fully connected layers with 4096 units and ReLU activation
VGGNet.add(Dense(units=3, activation="relu"))
VGGNet.add(Dense(units=3, activation="relu"))

# Add the output layer with 43 units and softmax activation for multi-class classification
VGGNet.add(Dense(units=3, activation="softmax"))

# Print the summary of the model
VGGNet.summary()
VGGNet.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])
VGGNet.evaluate(x_test, y_test, verbose=1)
history = VGGNet.fit(x_train, y_train, epochs=10)


import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import tensorflow as tf
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.applications import DenseNet201
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Dense
from keras.utils import to_categorical
import numpy as np
import copy
import random
import csv
x = np.array(x)
y=np.array(y)

# Assuming x and y are already defined

# Client config
NUM_OF_CLIENTS = 2
SELECT_CLIENTS = 0.3
EPOCHS = 5
CLIENT_EPOCHS = 5
BATCH_SIZE = 8
DROP_RATE = 0.5

# Define serverhist dictionary to store evaluation metrics
serverhist = {
    'loss': [],
    'accuracy': [],
    'f1': [],
    'precision': [],
    'recall': [],
    'specificity': [],
    'sensitivity': []
}

# Corrected code
evaluation = [7]
result = evaluation[0]
#print(result)

# Model config
LOSS = 'categorical_crossentropy'
NUM_OF_CLASSES = 3
learning_rate = 0.0025

# OPTIMIZER = SGD(lr=0.015, decay=0.01, nesterov=False)
OPTIMIZER = legacy.SGD(lr=lr, momentum=0.9, decay=lr/(EPOCHS*CLIENT_EPOCHS), nesterov=False) # lr = 0.015, 67 ~ 69%
serverhist={
    "loss": list(),
    "accuracy": list(),
    "payoff": list()
    }
class Model():

    def __init__(self, loss, optimizer, classes=10):
        self.loss = loss
        self.optimizer = optimizer
        self.num_classes = classes

    def fl_paper_model(self, train_shape):
        #model = Sequential()
        model = Sequential()

        # Create a sequential model
        VGGNet = Sequential()

        # Add the first convolutional layer with 64 filters, kernel size of 3x3, padding "same", and ReLU activation
        VGGNet.add(Conv2D(input_shape=(150,150,3), filters=96, kernel_size=(11, 11), padding="same", activation="relu"))

        # Add the second convolutional layer with 64 filters, kernel size of 3x3, padding "same", and ReLU activation
        VGGNet.add(Conv2D(filters=256, kernel_size=(5, 5), padding="same", activation="relu"))

        # Add the first max pooling layer with pool size 2x2 and stride 2x2
        VGGNet.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))

        # Continue adding more convolutional and max pooling layers with increasing number of filters
        VGGNet.add(Conv2D(filters=384, kernel_size=(3, 3), padding="same", activation="relu"))
        VGGNet.add(Conv2D(filters=128, kernel_size=(3, 3), padding="same", activation="relu"))
        VGGNet.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))

        VGGNet.add(Conv2D(filters=256, kernel_size=(3, 3), padding="same", activation="relu"))
        VGGNet.add(Conv2D(filters=256, kernel_size=(3, 3), padding="same", activation="relu"))
        VGGNet.add(Conv2D(filters=256, kernel_size=(3, 3), padding="same", activation="relu"))
        VGGNet.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))

        VGGNet.add(Conv2D(filters=512, kernel_size=(3, 3), padding="same", activation="relu"))
        VGGNet.add(Conv2D(filters=512, kernel_size=(3, 3), padding="same", activation="relu"))
        VGGNet.add(Conv2D(filters=512, kernel_size=(3, 3), padding="same", activation="relu"))
        VGGNet.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))

        VGGNet.add(Conv2D(filters=512, kernel_size=(3, 3), padding="same", activation="relu"))
        VGGNet.add(Conv2D(filters=512, kernel_size=(3, 3), padding="same", activation="relu"))
        VGGNet.add(Conv2D(filters=512, kernel_size=(3, 3), padding="same", activation="relu"))
        VGGNet.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))

       # Flatten the output from the previous layers
       VGGNet.add(Flatten())

       # Add two fully connected layers with 4096 units and ReLU activation
       VGGNet.add(Dense(units=3, activation="relu"))
       VGGNet.add(Dense(units=3, activation="relu"))

       # Add the output layer with 43 units and softmax activation for multi-class classification
       VGGNet.add(Dense(units=3, activation="softmax"))

       # Print the summary of the model
       VGGNet.summary()
       VGGNet.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])
       VGGNet.evaluate(x_test, y_test, verbose=1)
       history = VGGNet.fit(x_train, y_train, epochs=10)


def init_model(train_data_shape):
    model = Model(loss=LOSS, optimizer=OPTIMIZER, classes=NUMOFCLASSES)
    fl_model = model.fl_paper_model(train_shape=train_data_shape)

    return fl_model


def client_data_config(x_train, y_train):
    client_data = [() for _ in range(NUMOFCLIENTS)] # () for _ in range(NUMOFCLIENTS)
    num_of_each_dataset = int(x_train.shape[0] / NUMOFCLIENTS)

    for i in range(NUMOFCLIENTS):
        split_data_index = []
        while len(split_data_index) < num_of_each_dataset:
            item = random.choice(range(x_train.shape[0]))
            if item not in split_data_index:
                split_data_index.append(item)

        new_x_train = np.asarray([x_train[k] for k in split_data_index])
        new_y_train = np.asarray([y_train[k] for k in split_data_index])

        client_data[i] = (new_x_train, new_y_train)

    return client_data


# Define fedAVG function
def fedAVG(server_weight):
    avg_weight = np.array(server_weight[0])

    if len(server_weight) > 1:
        for i in range(1, len(server_weight)):
            avg_weight += server_weight[i]

        avg_weight = avg_weight / len(server_weight)

    return avg_weight

# Initialization
fl_model = init_model()
fl_model.summary()

# Split the data for each client
x_train_clients = np.array_split(x_train, NUM_OF_CLIENTS)
y_train_clients = np.array_split(y_train, NUM_OF_CLIENTS)
y_train_clients_one_hot = [to_categorical(labels) for labels in y_train_clients]

# Initialization
fl_model = init_model()
fl_model.summary()

# Split the data for each client
x_train_clients = np.array_split(x_train, NUM_OF_CLIENTS)
x_test_clients = np.array_split(x_test, NUM_OF_CLIENTS)
y_train_clients = np.array_split(y_train, NUM_OF_CLIENTS)
y_train_clients_one_hot = [to_categorical(labels) for labels in y_train_clients]

# Initialize avg_weight
avg_weight = fl_model.get_weights()

# Federated Learning Loop
for epoch in range(EPOCHS):
    client_models = [init_model() for _ in range(NUM_OF_CLIENTS)]

    selected_num = int(max(NUM_OF_CLIENTS * SELECT_CLIENTS, 1))
    split_data_index = random.sample(range(NUM_OF_CLIENTS), selected_num)

    x_test_clients = np.array_split(x_test, NUM_OF_CLIENTS)
    y_test_clients = np.array_split(y_test, NUM_OF_CLIENTS)
    # Initialize server_weight list inside the loop
    server_weight = []

    for index in split_data_index:
        client_models[index] = client_update(index, client_models[index], epoch, avg_weight)
        # Evaluate on the client's test set
        x_test_client = x_test_clients[index]
        y_test_client = to_categorical(y_test_clients[index], num_classes=3)

        client_evaluation = client_models[index].evaluate(x_test_client, y_test_client, batch_size=10, verbose=1)

        # Append the weights of the current client model to the server_weight list
        server_weight.append(copy.deepcopy(client_models[index].get_weights()))

    # Call fedAVG after updating all client models
    avg_weight = fedAVG(server_weight)
    fl_model.set_weights(avg_weight)

    #print("Server {}/{} evaluate".format(epoch + 1, EPOCHS))
    # Use x_test directly for evaluation
    y_test_int = np.array(y_test)  # Convert to numpy array if not already
    #evaluation = model.evaluate(x_test, y_test_categorical, batch_size=10, verbose=1)
    evaluation = fl_model.evaluate(x_test, y_test_categorical, batch_size=10, verbose=1)

    # Store the metrics in the serverhist dictionary
    serverhist['accuracy'].append(client_evaluation[1])  # Assuming accuracy is at index 1
    serverhist['loss'].append(client_evaluation[0])
    serverhist['f1'].append(client_evaluation[0])
    serverhist['precision'].append(client_evaluation[0])
    serverhist['recall'].append(client_evaluation[0])
    serverhist['specificity'].append(client_evaluation[0])
    serverhist['sensitivity'].append(client_evaluation[0])


# Print the results
print("\t\t\tFed Avg")
print("Round\tAccuracy\tLoss\tPrecision\tRecall\tF1\tSpecificity\tSensitivity")
for i in range(EPOCHS):
    print("{}\t{:.4f}\t{:.4f}\t{:.4f}\t{:.4f}\t{:.4f}\t{:.4f}\t{:.4f}".format(
        i + 1,
        serverhist['accuracy'][i],
        serverhist['loss'][i],
        serverhist['precision'][i],
        serverhist['recall'][i],
        serverhist['f1'][i],
        serverhist['specificity'][i],
        serverhist['sensitivity'][i]
    ))