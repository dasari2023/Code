{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGdgWX1cmrLA28/Qg3WXBM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dasari2023/Code/blob/main/Proposedalgorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzNQEwlnbbs5",
        "outputId": "7c69a01e-1ad1-4986-9dfe-7a0f0501504f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.2)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "import tensorflow\n",
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4GyiT5vbvhq",
        "outputId": "67a0eb18-9918-41f5-8218-c7a3aa1db6ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.backend import image_data_format\n",
        "from keras.models import Sequential\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from keras.models import Sequential\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Conv2D, DepthwiseConv2D, SeparableConv2D\n",
        "from keras.layers import AvgPool2D, MaxPooling2D, Dropout, Flatten, BatchNormalization\n",
        "#from keras.constraints import maxnorm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import copy\n",
        "import random\n",
        "import sys\n",
        "import glob\n",
        "import keras\n",
        "import cv2\n",
        "import csv\n",
        "import time\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "metadata": {
        "id": "yoXYehKtb0bC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pytesseract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9PFGUL6b8qR",
        "outputId": "8a24e293-36be-4d72-80bd-85c419e2ba2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "from pytesseract import image_to_string"
      ],
      "metadata": {
        "id": "soG1rPeJcAhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install opencv-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iV2rfBdQcEjC",
        "outputId": "7e20a049-2720-4d13-d756-2276ac976616"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x,y1, y = [], [],[]\n",
        "\n",
        "x_train=list()\n",
        "y_train=list()\n",
        "\n",
        "dgtrain =glob.glob('/content/drive/MyDrive/Dataset/Disease Grading/Training/*.jpg')\n",
        "ltrain =glob.glob('/content/drive/MyDrive/Dataset/Localization/Training/*.jpg')\n",
        "strain =glob.glob('/content/drive/MyDrive/Dataset/Segmentation/Training/*.jpg')\n",
        "for i in dgtrain:\n",
        "    img=cv2.imread(i,1)\n",
        "    img=cv2.resize(img,(224,224))\n",
        "    img=np.float32(img)\n",
        "    img/=255.0\n",
        "    img = img.reshape(224, 224, 3)\n",
        "    x.append(img)\n",
        "    y.append(0)\n",
        "for j in ltrain:\n",
        "    img=cv2.imread(j,1)\n",
        "    img=cv2.resize(img,(224,224))\n",
        "    img=np.float32(img)\n",
        "    img/=255.0\n",
        "    img = img.reshape(224, 224, 3)\n",
        "    x.append(img)\n",
        "    y.append(1)\n",
        "for k in strain:\n",
        "    img=cv2.imread(k,1)\n",
        "    img=cv2.resize(img,(224,224))\n",
        "    img=np.float32(img)\n",
        "    img/=255.0\n",
        "    img = img.reshape(224, 224, 3)\n",
        "    x.append(img)\n",
        "    y.append(2)"
      ],
      "metadata": {
        "id": "VWMICQ7IcITq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dgtest = glob.glob('/content/drive/MyDrive/Dataset/Disease Grading/Testing/*.jpg')\n",
        "\n",
        "ltest =glob.glob('/content/drive/MyDrive/Dataset/Localization/Testing/*.jpg')\n",
        "stest =glob.glob('/content/drive/MyDrive/Dataset/Segmentation/Testing/*.jpg')\n",
        "x_test=list()\n",
        "y_test=list()\n",
        "for i in dgtest:\n",
        "    img=cv2.imread(i,1)\n",
        "    img=cv2.resize(img,(224, 224))\n",
        "    img=np.float32(img)\n",
        "    img/=255.0\n",
        "    img = img.reshape(224, 224, 3)\n",
        "    x.append(img)\n",
        "    y.append(0)\n",
        "for j in ltest:\n",
        "    img=cv2.imread(j,1)\n",
        "    img=cv2.resize(img,(224, 224))\n",
        "    img=np.float32(img)\n",
        "    img/=255.0\n",
        "    img = img.reshape(224, 224, 3)\n",
        "    x.append(img)\n",
        "    y.append(1)\n",
        "for k in stest:\n",
        "    img=cv2.imread(k,1)\n",
        "    img=cv2.resize(img,(224, 224))\n",
        "    img=np.float32(img)\n",
        "    img/=255.0\n",
        "    img = img.reshape(224, 224, 3)\n",
        "    x.append(img)\n",
        "    y.append(2)"
      ],
      "metadata": {
        "id": "xYeBICi2cM3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define your dataset paths\n",
        "dgtrain = glob.glob('/content/drive/MyDrive/Dataset/Disease Grading/Training/*.jpg')\n",
        "ltrain = glob.glob('/content/drive/MyDrive/Dataset/Localization/Training/*.jpg')\n",
        "strain = glob.glob('/content/drive/MyDrive/Dataset/Segmentation/Training/*.jpg')\n",
        "\n",
        "dgtest = glob.glob('/content/drive/MyDrive/Dataset/Disease Grading/Testing/*.jpg')\n",
        "ltest = glob.glob('/content/drive/MyDrive/Dataset/Localization/Testing/*.jpg')\n",
        "stest = glob.glob('/content/drive/MyDrive/Dataset/Segmentation/Testing/*.jpg')\n",
        "\n",
        "# Function to load and preprocess images\n",
        "def load_and_preprocess_image(image_path):\n",
        "    img = cv2.imread(image_path, 1)\n",
        "    img = cv2.resize(img, (224, 224))\n",
        "    img = np.float32(img) / 255.0\n",
        "    img = img.reshape(224, 224, 3)\n",
        "    return img\n",
        "\n",
        "# Load and preprocess training data\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for i in dgtrain:\n",
        "    img = load_and_preprocess_image(i)\n",
        "    x.append(img)\n",
        "    y.append(0)\n",
        "\n",
        "for j in ltrain:\n",
        "    img = load_and_preprocess_image(j)\n",
        "    x.append(img)\n",
        "    y.append(1)\n",
        "\n",
        "for k in strain:\n",
        "    img = load_and_preprocess_image(k)\n",
        "    x.append(img)\n",
        "    y.append(2)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an ImageDataGenerator for augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Generate augmented images and append to the training set\n",
        "for i in range(len(x_train)):\n",
        "    img = x_train[i]\n",
        "    img = img.reshape((1,) + img.shape)  # Reshape to (1, height, width, channels) for flow method\n",
        "    for batch in datagen.flow(img, batch_size=1):\n",
        "        x_train.append(batch[0])\n",
        "        y_train.append(y_train[i])\n",
        "        break  # Exit the loop after one batch to avoid infinite loop\n",
        "\n",
        "# Similarly, you can perform data augmentation for validation data if needed\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "x_val = np.array(x_val)\n",
        "y_val = np.array(y_val)\n",
        "\n",
        "# Now, x_train and y_train contain augmented training data\n",
        "# x_val and y_val contain validation data\n",
        "# You can use these arrays to train your model\n"
      ],
      "metadata": {
        "id": "U_dXkYzJcRzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming you have loaded your data into the variables x and y correctly\n",
        "# x should contain the images, and y should contain the labels.\n",
        "\n",
        "# Convert data to NumPy arrays\n",
        "x_samp = np.asarray(x)\n",
        "y_samp = np.asarray(y)\n",
        "\n",
        "# Check the shapes of your data to ensure they match\n",
        "print(\"x_samp.shape:\", x_samp.shape)\n",
        "print(\"y_samp.shape:\", y_samp.shape)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_samp, y_samp, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check the shapes of the split datasets\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "#print(\"x_test shape:\", x_test.shape)\n",
        "#print(\"y_test shape:\", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3I35BW32cWR6",
        "outputId": "4bc039e9-a122-48d8-f50d-46d84d3d6a79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_samp.shape: (880, 224, 224, 3)\n",
            "y_samp.shape: (880,)\n",
            "x_train shape: (704, 224, 224, 3)\n",
            "y_train shape: (704,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYZpHokCcbVx",
        "outputId": "ffbcac25-a56a-41b8-84ed-78214720a644"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "#proposed algorithm implementation\n",
        "\n",
        "def initialize_global_model():\n",
        "    # Function to initialize the global model weights\n",
        "    # Placeholder implementation; replace with actual model initialization\n",
        "    global_model = None\n",
        "    return global_model\n",
        "\n",
        "def randomly_select_clients():\n",
        "    # Function to randomly select clients for this iteration\n",
        "    # Placeholder implementation; replace with actual client selection logic\n",
        "    selected_clients = []\n",
        "    return selected_clients\n",
        "\n",
        "def average_weights(weights, selected_clients):\n",
        "    # Function to aggregate client weights\n",
        "    # Placeholder implementation; replace with actual weight averaging logic\n",
        "    averaged_weights = None\n",
        "    return averaged_weights\n",
        "\n",
        "def train_on_client(client, w):\n",
        "    # Function to continue learning process on client with weight w\n",
        "    # Placeholder implementation; replace with actual client training logic\n",
        "    updated_weight = None\n",
        "    return updated_weight\n",
        "\n",
        "def evaluate_model(w, local_data):\n",
        "    # Function to evaluate global model w using local data\n",
        "    # Placeholder implementation; replace with actual model evaluation logic\n",
        "    evaluation_results = None\n",
        "    return evaluation_results\n",
        "\n",
        "def SERVERAGGREGATION(eta, N, max_iterations, all_clients):\n",
        "    # Initialize global model variables: w_0, w_1, ..., w_n\n",
        "    global_model = initialize_global_model()\n",
        "\n",
        "    for a in range(1, max_iterations + 1):\n",
        "        # Randomly select clients for this iteration\n",
        "        S_a = randomly_select_clients()\n",
        "\n",
        "        # Update client models in parallel\n",
        "        client_weights = []\n",
        "        for t in S_a:\n",
        "            w_k_a_plus_1 = UPDATECLIENT(t, global_model)\n",
        "            client_weights.append(w_k_a_plus_1)\n",
        "\n",
        "        # Aggregate client weights\n",
        "        global_model = average_weights(client_weights, S_a)\n",
        "\n",
        "    # Evaluate the new global model\n",
        "    evaluation_results = []\n",
        "    for t in all_clients:\n",
        "        y_t = EVALUATE(global_model, t['local_data'])\n",
        "        evaluation_results.append(y_t)\n",
        "\n",
        "    return global_model, evaluation_results\n",
        "\n",
        "def UPDATECLIENT(client, w):\n",
        "    # Continue learning process on client with weight w\n",
        "    # until client completes the task (reaches E epochs)\n",
        "    # Update weight accordingly after learning phase\n",
        "    updated_weight = train_on_client(client, w)\n",
        "    return updated_weight\n",
        "\n",
        "def EVALUATE(w, local_data):\n",
        "    # Evaluate global model w using local data\n",
        "    evaluation_results = evaluate_model(w, local_data)\n",
        "    return evaluation_results"
      ],
      "metadata": {
        "id": "JZvNGzQcYOOl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}